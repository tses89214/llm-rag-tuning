{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "     api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: provide info in the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In method 1, we just put the information in the prompt and let the model generate the answer.\n",
    "\n",
    "Our test question is:\n",
    "```\n",
    "I want to buy a new model, which is good for my family. \n",
    "I have 2 children and always travel with my wife and her parents, \n",
    "so I want to make sure that the model is suitable for all of them.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your requirements, I recommend the Stark 4. It is a family-friendly spaceship with ample space and comfort for long voyages. Here are some details that make it suitable for your family:\n",
      "\n",
      "- **Capacity**: Accommodates up to 12 passengers, which is more than sufficient for your family of six.\n",
      "- **Size**: 35 meters in length, providing ample space for comfort.\n",
      "- **Compatible Accessories**: Includes options for an Entertainment System and Extra Sleeping Pods, ensuring a comfortable journey for all family members.\n",
      "- **Notes**: Ensure you check the oxygen recycling system functionality before any journey exceeding 10 days, ensuring safety during longer trips.\n",
      "\n",
      "The Stark 4 is priced at $8,000,000, featuring an Ocean Blue color that adds a serene touch to your travels. Let me know if you need any more information or assistance!\n"
     ]
    }
   ],
   "source": [
    "def generate_base_prompt():\n",
    "    prompt = \\\n",
    "    \"\"\"\n",
    "    You are a customer service chatbot. \n",
    "    use following models' information to assist the user:\\n\n",
    "    \"\"\"\n",
    "    model_list = [f\"model_{n}\" for n in range(1, 11)]\n",
    "    for model in model_list:\n",
    "        with open(f\"data/stark_spaceships/{model}.txt\", \"r\") as f:\n",
    "            content = f.read()\n",
    "        prompt += f\"- {content}\\n\"\n",
    "    prompt += \"\\nHow can I assist you today? please provide your ship's model.\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def get_gpt_response(prompt, user_input):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": user_input}\n",
    "            ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def main():\n",
    "    prompt = generate_base_prompt()\n",
    "    user_input = \"\"\"\n",
    "    I want to buy a new model, which is good for my family. \n",
    "    I have 2 children and always travel with my wife and her parents, \n",
    "    and I want to make sure that the model is suitable for all of them.\n",
    "    \"\"\"\n",
    "    print(get_gpt_response(prompt, user_input))\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Method 1, we just put info in the prompt,we can find some pros and cons:\n",
    "- pros:\n",
    "  - Simple Implementation: You can include the information directly in the prompt without extra data processing or adjustments.\n",
    "\n",
    "  - Immediate Update: Product information can be quickly changed or updated as needed.\n",
    "- cons:\n",
    "  - Context Limitations: The length of the prompt is limited; including too much information may exceed the token limit, requiring simplification or risking truncation.\n",
    "\n",
    "  - Increased Cost: Longer prompts increase the cost of API calls, as pricing is often based on the number of tokens used.\n",
    "\n",
    "  - Redundancy: If some information isn't always needed, it might unnecessarily increase processing time and response latency.\n",
    "\n",
    "  - Reduced Flexibility: Fixed information may not easily adapt to specific queries, lacking dynamic adjustment based on the particular question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2:  Use RAG method to provide information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our documents are too numerous to fit into a prompt all at once, we typically need to implement a search method to retrieve the documents that most closely match the user's query. We then provide the content of these documents to GPT for integration and summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use BERT(all-MiniLM-L6-v2) as the embedding algorithm to retrieve document information that closely matches the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Relevant Documents: ['model 2']\n",
      "Response: The Stark 2 is a reliable spacecraft for adventurers and is compatible with extra fuel tanks.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "# Load BERT model for encoding\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def load_product_data():\n",
    "    docs = {}\n",
    "    for file in os.listdir(\"data/stark_spaceships\"):\n",
    "        docs[file.replace(\".txt\", \"\").replace(\"_\", \" \")] = \\\n",
    "            open(f\"data/stark_spaceships/{file}\").read()\n",
    "    return docs\n",
    "\n",
    "def create_embeddings(document_texts):\n",
    "    \"\"\"Encode all document texts into BERT embeddings\"\"\"\n",
    "    document_embeddings = model.encode(\n",
    "        list(document_texts.values()), convert_to_tensor=True)\n",
    "    return document_embeddings\n",
    "\n",
    "\n",
    "def find_most_relevant_documents(query, document_texts, document_embeddings, top_n=1):\n",
    "    \"\"\"Encode the query to BERT embedding\"\"\"\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarity between the query and each document\n",
    "    similarities = util.pytorch_cos_sim(\n",
    "        query_embedding, document_embeddings)[0]\n",
    "\n",
    "    # Get the indices of the top N most similar documents\n",
    "    most_relevant_indices = np.argsort(similarities)[-top_n:]\n",
    "\n",
    "    # Retrieve document names\n",
    "    most_relevant_docs = [list(document_texts.keys())[index] for index in most_relevant_indices]\n",
    "\n",
    "    return most_relevant_docs\n",
    "\n",
    "def generate_response(query, docs):\n",
    "    \"\"\"put docs into a prompt and generate response\"\"\"\n",
    "    \n",
    "    prompt = f\"Answer the following question based on the provided documents:\\n\\n{query}\\n\\nDocuments:\\n\"\n",
    "    for doc in docs:\n",
    "        prompt += f\"{doc}\\n\"\n",
    "    prompt += \"Answer:\"\n",
    "    \n",
    "    # call gpt-4o\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def main():\n",
    "    # user query\n",
    "    query = \"What spacecraft is reliable for adventurers and has extra fuel tanks?\"\n",
    "    # TODO: in this step, sometimes we need to transform the query some more specific keywords, we just skip here.\n",
    "\n",
    "    # Create emdeddings\n",
    "    docs = load_product_data()\n",
    "    embeddings = create_embeddings(docs)\n",
    "    \n",
    "    # Find most relevant documents\n",
    "    relevant_docs_key = find_most_relevant_documents(query, docs, embeddings)\n",
    "    print(\"Most Relevant Documents:\", relevant_docs_key)\n",
    "    \n",
    "    # generate response with docs\n",
    "    relevant_docs = [docs[index] for index in relevant_docs_key]\n",
    "    print(\"Response:\", generate_response(query, relevant_docs))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In method 2, we use RAG to make Chatbot can answer questions about Stark Spaceships.\n",
    "\n",
    "- Pros\n",
    "  - Contextual Completeness: RAG allows a model to answer questions based on specific, detailed context from retrieved documents, improving accuracy and relevance.\n",
    "\n",
    "  - Scalability: Efficiently handles large datasets by retrieving only relevant portions, which helps in scaling to large corpora of information.\n",
    "\n",
    "  - Dynamic Information Update: As the document database is updated, the retrieval component can access the most up-to-date information without needing to retrain the model.\n",
    "\n",
    "  - Cost-Effective: Reduces the need for extensive fine-tuning or retraining by leveraging existing powerful models with up-to-date information retrieval.\n",
    "\n",
    "  - Domain Adaptability: Easily integrate information from various domains simply by updating the underlying document corpus, without needing modifications to the generation model.\n",
    "\n",
    "- Cons\n",
    "  -  Complex Implementation: Setting up a RAG system involves implementing both a retrieval mechanism and a generation model, which can be complex and require significant engineering work.\n",
    "\n",
    "  - Latency: Adding a retrieval step can increase response time, which may impact performance in real-time applications.\n",
    "\n",
    "  - Dependency on Document Quality: The effectiveness of RAG is highly dependent on the quality and relevance of the documents in the retrieval database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also use fine-tuning to improve the model's performance on a specific task. In this case, we'll fine-tune the model on the task of generating a polite answer to customers when they complaint about their experience with our product, and ask them to information to our customer service team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning job status: validating_files\n",
      "Fine-tuning job status: validating_files\n",
      "Fine-tuning job status: validating_files\n",
      "Fine-tuning job status: running\n",
      "Fine-tuning job status: running\n",
      "Fine-tuning job status: running\n",
      "Fine-tuning job status: running\n",
      "Fine-tuning job status: running\n",
      "Fine-tuning job status: running\n",
      "Fine-tuning job status: running\n",
      "Fine-tuning job status: running\n",
      "Fine-tuning job status: running\n",
      "Fine-tuning job status: running\n",
      "Fine-tuning job status: running\n",
      "Fine-tuning job status: running\n",
      "Fine-tuning job status: running\n",
      "Fine-tuning job succeeded.\n",
      "This is concerning, and we deeply regret the error. Please email customer_service@stack_spaceship.com with your order details, and we will arrange for the correct model to be sent to you.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "def upload_file(file_name, purpose):\n",
    "    with open(file_name, \"rb\") as file_fd:\n",
    "        response = client.files.create(file=file_fd, purpose=purpose)\n",
    "    return response.id\n",
    "\n",
    "\n",
    "def run_fine_tuning_job(file_id):\n",
    "    \"\"\"\n",
    "    Create and start a fine-tuning job.\n",
    "    \"\"\"\n",
    "    client.fine_tuning.jobs.create(\n",
    "        training_file=file_id,\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "    )\n",
    "    \n",
    "    task_id = client.fine_tuning.jobs.list(limit=1).data[0].id\n",
    "    while True:\n",
    "        status = client.fine_tuning.jobs.retrieve(task_id).status\n",
    "        if status in (\"failed\", \"canceled\"):\n",
    "            print(f\"Fine-tuning job failed with status: {status}\")\n",
    "            break\n",
    "        elif status == \"succeeded\":\n",
    "            print(f\"Fine-tuning job succeeded.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Fine-tuning job status: {status}\")\n",
    "            time.sleep(30)\n",
    "    \n",
    "    fine_tuned_model = client.fine_tuning.jobs.retrieve(task_id).fine_tuned_model\n",
    "    return fine_tuned_model\n",
    "        \n",
    "def delete_model(model_id):\n",
    "    client.models.delete(model_id)\n",
    "\n",
    "def chat_completion(user_input, model_id):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def main(user_input):\n",
    "    file_id = upload_file(\"data/fine-tuning-job-example.jsonl\", \"fine-tune\")\n",
    "    fine_tuned_model_id = run_fine_tuning_job(file_id)\n",
    "    print(chat_completion(user_input, fine_tuned_model_id))\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = \"I received the wrong model! How could this happen?\"\n",
    "    main(user_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
